{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep_learning_assignment.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aobo-y/uva-cv/blob/master/deep_learning_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "z-__RAO5t53G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Deep Learning Basics\n",
        "\n",
        "In this tutorial we we will implement a toy deep learning framework, to rapidly create neural network models. It should have some flexibilty and will be implemented so that new modules can be added and replaced just like in modern deep learning frameworks. We will be implementing some types of basic layers. We will also explore pytorch's nn package and see how pytorch accomplishes this goal and what are the extras that pytorch offers.\n"
      ]
    },
    {
      "metadata": {
        "id": "Y5AUSc3jufKi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1. Cleaner mini-batch-capable Linear + Softmax classifier\n"
      ]
    },
    {
      "metadata": {
        "id": "Vby-ogCCxYwa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the previous assignment, we provided a basic implementation of a linear softmax classifier. In this assignment we will use this as a starting point for our deep learning library. \n",
        "\n",
        "**Softmax + Negative Log-Likelihood:** First we will re-implement the (softmax + negative log likelihood loss) computation, and its gradient computation. But we will additionally support batches of inputs and labels. \n"
      ]
    },
    {
      "metadata": {
        "id": "j0JJRfD6ti7m",
        "colab_type": "code",
        "outputId": "813d8e59-9b49-42aa-8a9c-594fdbec81c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# This class combines Softmax + Negative-log likelihood loss.\n",
        "# Similar to the previous lab, but this implementation works for \n",
        "# batches of inputs and not just individual input vectors. \n",
        "# Here \"inputs\" is batchSize x sizePredictionScores, and\n",
        "#      \"labels\" is a vector of size batchSize.\n",
        "class toynn_CrossEntropyLoss(object): \n",
        "  \n",
        "    # Forward pass: -log softmax(input_{label})\n",
        "    def forward(self, scores, labels):\n",
        "      \n",
        "        # 1. Computing the softmax: exp(x) / sum (exp(x))\n",
        "        max_val = scores.max()  # This is to avoid variable overflows.\n",
        "        exp_inputs = (scores - max_val).exp()\n",
        "        # This is different than in the previous lab. Avoiding for loops here.\n",
        "        denominators = exp_inputs.sum(1).repeat(scores.size(1), 1).t()\n",
        "        self.predictions = torch.mul(exp_inputs, 1 / denominators)\n",
        "        \n",
        "        # 2. Computing the loss: -log(y_label).\n",
        "        # Check what gather does. Just avoiding another for loop here.\n",
        "        return -self.predictions.log().gather(1, labels.view(-1, 1)).mean()\n",
        "    \n",
        "    # Backward pass: y_hat - y\n",
        "    def backward(self, scores, labels):\n",
        "      \n",
        "        # Here we avoid computing softmax again in backward pass.\n",
        "        grad_inputs = self.predictions.clone()\n",
        "        \n",
        "        # Ok, Here we will use a for loop (but it is avoidable too).\n",
        "        for i in range(0, scores.size(0)):\n",
        "            grad_inputs[i][labels[i]] = grad_inputs[i][labels[i]] - 1\n",
        "            \n",
        "        return grad_inputs \n",
        "      \n",
        "      \n",
        "# Let's verify if the above seems to be Okay.\n",
        "batchSize = 32\n",
        "mock_scores = torch.zeros(batchSize, 10).normal_(0, 0.1)\n",
        "mock_labels = torch.zeros(batchSize, 1, dtype=torch.long).fill_(3)\n",
        "\n",
        "loss_fn = toynn_CrossEntropyLoss()\n",
        "\n",
        "loss = loss_fn.forward(mock_scores, mock_labels)\n",
        "mock_scores_grads = loss_fn.backward(mock_scores, mock_labels)\n",
        "\n",
        "print(\"Input predictions: \", mock_scores.shape)\n",
        "print(\"Input labels: \", mock_labels.shape)\n",
        "print(\"Output loss: \", loss.item())\n",
        "print(\"Input prediction gradients: \", mock_scores_grads.shape)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input predictions:  torch.Size([32, 10])\n",
            "Input labels:  torch.Size([32, 1])\n",
            "Output loss:  2.3008241653442383\n",
            "Input prediction gradients:  torch.Size([32, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-DfaJM7RWk_0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Linear Transformation:**  Next, we will re-implement the linear transformation computation $y=Wx+b$, and its gradient computation. But we will additionally support batches of inputs and labels. Making a batched implementation of this layer is easier because the only change is that now we have matrix-matrix multiplications as opposed to vector-matrix multiplications. Additionally, we will support returning the gradients with respect to the inputs to the linear transformation ($\\partial \\ell / \\partial x_j$). Notice that in our previous assignment we were only concerned with computing $\\partial \\ell / \\partial w_{ij}$ and  $\\partial \\ell / \\partial b_i$ (gradients for the parameters)."
      ]
    },
    {
      "metadata": {
        "id": "2Lfp8zLNXVo_",
        "colab_type": "code",
        "outputId": "3f2c9c85-e3f5-4a1c-e480-72df13d22bfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "class toynn_Linear(object):\n",
        "    def __init__(self, numInputs, numOutputs):\n",
        "        # Allocate tensors for the weight and bias parameters.\n",
        "        self.weight = torch.Tensor(numInputs, numOutputs).normal_(0, 0.01)\n",
        "        self.weight_grads = torch.Tensor(numInputs, numOutputs)\n",
        "        self.bias = torch.Tensor(numOutputs).zero_()\n",
        "        self.bias_grads = torch.Tensor(numOutputs)\n",
        "    \n",
        "    # Forward pass, inputs is a matrix of size batchSize x numInputs.\n",
        "    # Notice that compared to the previous assignment, each input vector\n",
        "    # is a row in this matrix.\n",
        "    def forward(self, inputs):\n",
        "        # This one needs no change, it just becomes \n",
        "        # a matrix x matrix multiplication\n",
        "        # as opposed to just vector x matrix multiplication as we had before.\n",
        "        return torch.matmul(inputs, self.weight) + self.bias\n",
        "    \n",
        "    # Backward pass, in addition to compute gradients for the weight and bias.\n",
        "    # It has to compute gradients with respect to inputs. \n",
        "    def backward(self, inputs, scores_grads):\n",
        "        self.weight_grads = torch.matmul(inputs.t(), scores_grads)\n",
        "        self.bias_grads = scores_grads.sum(0)\n",
        "        return torch.matmul(scores_grads, self.weight.t())\n",
        "\n",
        "# Input: batchSize x numInputs.\n",
        "numInputs = 1 * 28 * 28\n",
        "mock_inputs = torch.Tensor(batchSize, numInputs).normal_(0, 0.1)\n",
        "\n",
        "# Create the linear object to use.\n",
        "linear = toynn_Linear(numInputs, 10)\n",
        "\n",
        "# Forward and Backward passes:\n",
        "scores = linear.forward(mock_inputs)\n",
        "mock_inputs_grads = linear.backward(mock_inputs, mock_scores_grads)\n",
        "\n",
        "print(\"Input x: \", mock_inputs.shape)\n",
        "print(\"Weights W: \", linear.weight.shape)\n",
        "print(\"Biases b: \", linear.bias.shape)\n",
        "print(\"Outputs: \", scores.shape)\n",
        "print(\"dL / dx: \", mock_inputs_grads.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input x:  torch.Size([32, 784])\n",
            "Weights W:  torch.Size([784, 10])\n",
            "Biases b:  torch.Size([10])\n",
            "Outputs:  torch.Size([32, 10])\n",
            "dL / dx:  torch.Size([32, 784])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NNiPMoctZ5T_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We are finished with a cleaner implementation of the linear + softmax + negative log-likelihood classifier that we implemented for the previous assignment: (1) It supports batches,  (2) the functions for forward and backward are nicely packaged in a python class, (3) the weight and bias matrices (as well as weight_grad and bias_grad matrices) are nicely created and initialized in the constructor of the toynn_Linear class, (4) the Linear class also computes $\\partial \\ell / \\partial x_j$'s, which will be useful to stack layers in order to train deeper models."
      ]
    },
    {
      "metadata": {
        "id": "PGdZ-bmxb473",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2. Mini-batch SGD on FashionMNIST\n"
      ]
    },
    {
      "metadata": {
        "id": "zUfYXo8McA14",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Given the newly implemented toynn_CrossEntropyLoss and toynn_Linear classes, let's train a classifier which is exactly the same as in the previous assignment, but now we can use batches of examples, as opposed to single examples during training with Stochastic (mini-batch) Gradient Descent (SGD). There are a few changes to make to the code from the previous assignment"
      ]
    },
    {
      "metadata": {
        "id": "IJDmV7Xgb3RI",
        "colab_type": "code",
        "outputId": "ccf8c4ee-f507-414d-e1e8-47faf80c746c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import FashionMNIST\n",
        "\n",
        "# Removes, the need to call F.to_image ourselves.\n",
        "# Also, please look up what transforms.Normalize does.\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.5, 0.5, 0.5), \n",
        "                                                    (0.5, 0.5, 0.5))])\n",
        "\n",
        "# Load the training, and validation datasets.\n",
        "trainset = FashionMNIST(root = './data', train = True, transform = transform, download = True)\n",
        "valset = FashionMNIST(root = './data', train = False, transform = transform, download = True)\n",
        "\n",
        "# NEW: Pytorch DataLoader for iterating over batches.\n",
        "batchSize = 100\n",
        "\n",
        "# Shuffling is needed in case dataset is not shuffled by default.\n",
        "train_loader = torch.utils.data.DataLoader(dataset = trainset,\n",
        "                                           batch_size = batchSize,\n",
        "                                           shuffle = True)\n",
        "# We don't need to bach the validation set but let's do it anyway.\n",
        "val_loader = torch.utils.data.DataLoader(dataset = valset,\n",
        "                                         batch_size = batchSize,\n",
        "                                         shuffle = False) # No need.\n",
        "\n",
        "# Define a learning rate. \n",
        "learningRate = 1e-4\n",
        "\n",
        "# Define number of epochs.\n",
        "N = 5\n",
        "\n",
        "# Create the model.\n",
        "loss_fn = toynn_CrossEntropyLoss()\n",
        "linear_fn = toynn_Linear(1 * 28 * 28, 10)\n",
        "\n",
        "\n",
        "# log accuracies and losses.\n",
        "train_accuracies = []; val_accuracies = []\n",
        "train_losses = []; val_losses = []\n",
        "\n",
        "# Training loop. Please make sure you understand every single line of code below.\n",
        "# Go back to some of the previous steps in this lab if necessary.\n",
        "for epoch in range(0, N):\n",
        "    correct = 0.0\n",
        "    cum_loss = 0.0\n",
        "    \n",
        "    # Make a pass over the training data.\n",
        "    for (i, (inputs, labels)) in enumerate(train_loader):\n",
        "        inputs = inputs.view(batchSize, 1 * 28 * 28)\n",
        "        \n",
        "        # Forward pass. (Prediction stage)\n",
        "        scores = linear_fn.forward(inputs)\n",
        "        cum_loss += loss_fn.forward(scores, labels).item()\n",
        "        \n",
        "        # Count how many correct in this batch.\n",
        "        max_scores, max_labels = scores.max(1)\n",
        "        correct += (max_labels == labels).sum().item()\n",
        "        \n",
        "        #Backward pass. (Gradient computation stage)\n",
        "        scores_grads = loss_fn.backward(scores, labels)\n",
        "        grad_inputs = linear_fn.backward(inputs, scores_grads)\n",
        "        \n",
        "        # Parameter updates (SGD step).\n",
        "        linear_fn.weight.add_(-learningRate, linear_fn.weight_grads)\n",
        "        linear_fn.bias.add_(-learningRate, linear_fn.bias_grads)\n",
        "        \n",
        "        # Logging the current results on training.\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print('Train-epoch %d. Iteration %05d, Avg-Loss: %.4f, Accuracy: %.4f' % \n",
        "                  (epoch, i + 1, cum_loss / (i + 1), correct / ((i + 1) * batchSize)))\n",
        "    \n",
        "    train_accuracies.append(correct / len(trainset))\n",
        "    train_losses.append(cum_loss / len(trainset))\n",
        "    \n",
        "    \n",
        "    # Make a pass over the validation data.\n",
        "    correct = 0.0\n",
        "    cum_loss = 0.0\n",
        "    for (i, (inputs, labels)) in enumerate(val_loader):\n",
        "        inputs = inputs.view(batchSize, 1 * 28 * 28)\n",
        "        \n",
        "        # Forward pass. (Prediction stage)\n",
        "        scores = linear_fn.forward(inputs)\n",
        "        cum_loss += loss_fn.forward(scores, labels).item()\n",
        "        \n",
        "         # Count how many correct in this batch.\n",
        "        max_scores, max_labels = scores.max(1)\n",
        "        correct += (max_labels == labels).sum().item()\n",
        "          \n",
        "    val_accuracies.append(correct / len(valset))\n",
        "    val_losses.append(cum_loss / (i + 1))\n",
        "            \n",
        "    # Logging the current results on validation.\n",
        "    print('Validation-epoch %d. Avg-Loss: %.4f, Accuracy: %.4f' % \n",
        "          (epoch, cum_loss / (i + 1), correct / len(valset)))"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train-epoch 0. Iteration 00100, Avg-Loss: 1.1423, Accuracy: 0.6796\n",
            "Train-epoch 0. Iteration 00200, Avg-Loss: 0.9524, Accuracy: 0.7171\n",
            "Train-epoch 0. Iteration 00300, Avg-Loss: 0.8609, Accuracy: 0.7368\n",
            "Train-epoch 0. Iteration 00400, Avg-Loss: 0.8038, Accuracy: 0.7497\n",
            "Train-epoch 0. Iteration 00500, Avg-Loss: 0.7652, Accuracy: 0.7594\n",
            "Train-epoch 0. Iteration 00600, Avg-Loss: 0.7376, Accuracy: 0.7661\n",
            "Validation-epoch 0. Avg-Loss: 0.6006, Accuracy: 0.7937\n",
            "Train-epoch 1. Iteration 00100, Avg-Loss: 0.5697, Accuracy: 0.8069\n",
            "Train-epoch 1. Iteration 00200, Avg-Loss: 0.5719, Accuracy: 0.8054\n",
            "Train-epoch 1. Iteration 00300, Avg-Loss: 0.5606, Accuracy: 0.8087\n",
            "Train-epoch 1. Iteration 00400, Avg-Loss: 0.5538, Accuracy: 0.8115\n",
            "Train-epoch 1. Iteration 00500, Avg-Loss: 0.5520, Accuracy: 0.8123\n",
            "Train-epoch 1. Iteration 00600, Avg-Loss: 0.5474, Accuracy: 0.8142\n",
            "Validation-epoch 1. Avg-Loss: 0.5457, Accuracy: 0.8100\n",
            "Train-epoch 2. Iteration 00100, Avg-Loss: 0.5287, Accuracy: 0.8177\n",
            "Train-epoch 2. Iteration 00200, Avg-Loss: 0.5259, Accuracy: 0.8185\n",
            "Train-epoch 2. Iteration 00300, Avg-Loss: 0.5156, Accuracy: 0.8226\n",
            "Train-epoch 2. Iteration 00400, Avg-Loss: 0.5102, Accuracy: 0.8262\n",
            "Train-epoch 2. Iteration 00500, Avg-Loss: 0.5099, Accuracy: 0.8262\n",
            "Train-epoch 2. Iteration 00600, Avg-Loss: 0.5091, Accuracy: 0.8263\n",
            "Validation-epoch 2. Avg-Loss: 0.5229, Accuracy: 0.8222\n",
            "Train-epoch 3. Iteration 00100, Avg-Loss: 0.4937, Accuracy: 0.8342\n",
            "Train-epoch 3. Iteration 00200, Avg-Loss: 0.4881, Accuracy: 0.8340\n",
            "Train-epoch 3. Iteration 00300, Avg-Loss: 0.4901, Accuracy: 0.8327\n",
            "Train-epoch 3. Iteration 00400, Avg-Loss: 0.4882, Accuracy: 0.8332\n",
            "Train-epoch 3. Iteration 00500, Avg-Loss: 0.4891, Accuracy: 0.8325\n",
            "Train-epoch 3. Iteration 00600, Avg-Loss: 0.4884, Accuracy: 0.8325\n",
            "Validation-epoch 3. Avg-Loss: 0.5119, Accuracy: 0.8220\n",
            "Train-epoch 4. Iteration 00100, Avg-Loss: 0.4849, Accuracy: 0.8358\n",
            "Train-epoch 4. Iteration 00200, Avg-Loss: 0.4768, Accuracy: 0.8376\n",
            "Train-epoch 4. Iteration 00300, Avg-Loss: 0.4747, Accuracy: 0.8380\n",
            "Train-epoch 4. Iteration 00400, Avg-Loss: 0.4743, Accuracy: 0.8379\n",
            "Train-epoch 4. Iteration 00500, Avg-Loss: 0.4761, Accuracy: 0.8372\n",
            "Train-epoch 4. Iteration 00600, Avg-Loss: 0.4746, Accuracy: 0.8382\n",
            "Validation-epoch 4. Avg-Loss: 0.4986, Accuracy: 0.8236\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NheOKVEvktSU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We achieved an accuracy of 80% a lot faster than in the previous assignment, in fact we reached almost 83% in the same amount of epochs as before, so it seems mini-batching is helping to some extent. Try experimenting with different batch sizes and learning rates. Batch-size and learning rate are two hyper-parameters that are related in the optimization process, and is a current line of active research. For instance, under deep learning models, larger batch sizes do not offer as good generalization as smaller batches, which is bad because it is easier to parallelize code when using larger batch sizes. Also, a common strategy during training is to reduce (decay) the learning rate, as the training progresses to the later epochs, but a recent paper proposed to increase the batch size instead (for more, see: https://arxiv.org/abs/1711.00489 ). Probably for small datasets, keeping the learning rate fixed and batch size fixed will be fine, but as one moves to larger datasets, and deeper models, it becomes crucial to use some more advanced strategies."
      ]
    },
    {
      "metadata": {
        "id": "rL8RK38atNE5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3. The Rectified Linear Unit (ReLU) Activation Function.\n"
      ]
    },
    {
      "metadata": {
        "id": "w2SukZfmtbH_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We are close to implementing a neural network, we can accomplish this by stacking a linear operation on top of a Rectified Linear Unit (ReLU) activation, another linear operation, and the softmax + negative log likelihood loss. This is all it takes to create simple neural network with one hidden layer. First, let's implement the ReLU layer as we implemented the linear layer. "
      ]
    },
    {
      "metadata": {
        "id": "i6ZplFBMwAkD",
        "colab_type": "code",
        "outputId": "da47b333-3af1-434b-dd14-9b1eb6e872ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "class toynn_ReLU(object):\n",
        "  \n",
        "    # Forward operation: f(x_i) = max(0, x_i)\n",
        "    def forward(self, inputs):\n",
        "        outputs = inputs.clone()\n",
        "        outputs[outputs < 0] = 0\n",
        "        return outputs\n",
        "    \n",
        "    # Make sure the backward pass is absolutely clear.\n",
        "    def backward(self, inputs, outputs_grad):\n",
        "        inputs_grad = outputs_grad.clone() # 1 * previous_grads\n",
        "        inputs_grad[inputs < 0] = 0  # or zero.\n",
        "        return inputs_grad\n",
        "      \n",
        "# Let's test it.\n",
        "x = torch.tensor([-2.3, 2.3, 3.1, -1.3, 4.3])\n",
        "relu_fn = toynn_ReLU()\n",
        "\n",
        "print(\"Input x: \", x)\n",
        "print(\"Output: \", relu_fn.forward(x))\n",
        "print(\"Grad x: \", relu_fn.backward(x, torch.ones(5)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input x:  tensor([-2.3000,  2.3000,  3.1000, -1.3000,  4.3000])\n",
            "Output:  tensor([0.0000, 2.3000, 3.1000, 0.0000, 4.3000])\n",
            "Grad x:  tensor([0., 1., 1., 0., 1.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ajJ1z9qb0oqt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 4. Forward Pass in a Two-Layer Neural Network"
      ]
    },
    {
      "metadata": {
        "id": "LK4M717G0v-8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We are going to show here how to perform inference in a two-layer neural network using the operations defined earlier. In the questions for the assignment it is your task to train this network and demonstrate superior accuracy."
      ]
    },
    {
      "metadata": {
        "id": "pH4e6WI21DHw",
        "colab_type": "code",
        "outputId": "928aa9d8-3498-4072-b832-ab5b71fdf63e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib\n",
        "matplotlib.rc('image', cmap = 'gray')\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Setup the input variable x.\n",
        "img, label = trainset[0]\n",
        "x = img.view(1, 1 * 28 * 28)\n",
        "\n",
        "# Setup the number of inputs, hidden neurons, and outputs.\n",
        "nInputs = 1 * 28 * 28\n",
        "nHidden = 512\n",
        "nOutputs = 10\n",
        "\n",
        "# Create the model here.\n",
        "linear_fn1 = toynn_Linear(nInputs, nHidden)\n",
        "relu_fn = toynn_ReLU()\n",
        "linear_fn2 = toynn_Linear(nHidden, nOutputs)\n",
        "\n",
        "# Make predictions.\n",
        "a = linear_fn1.forward(x)\n",
        "z = relu_fn.forward(a)\n",
        "yhat = linear_fn2.forward(z)\n",
        "\n",
        "# Show the prediction scores for each class.\n",
        "# Yes, pytorch tensors already come with a softmax function.\n",
        "# We need it here because we hard-coded the softmax inside \n",
        "# the loss function.\n",
        "print(yhat.softmax(dim = 1)) \n",
        "\n",
        "plt.imshow(img[0]); plt.axis('off'); plt.grid(False)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.1027, 0.1031, 0.1051, 0.0927, 0.0937, 0.1048, 0.1005, 0.0997, 0.1037,\n",
            "         0.0942]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFKCAYAAACU6307AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAADIFJREFUeJzt3bmvlvW3xuFnAzIKqCAR1IhGiTYG\nI85DolGjncEEW0No7P0PLDRa21laa2GcYg9GiTFQ0AjOGpyQMM+nOc05ydnn84W9QX9eV+ud9fIO\n3DyFK2vmwoULFyYAZrXgSv8BAP4JlCVAoCwBAmUJEChLgEBZAgSLLseLzMzMXI6XAbgks/2flJ4s\nAQJlCRAoS4BAWQIEyhIgUJYAgbIECJQlQKAsAQJlCRAoS4BAWQIEyhIgUJYAgbIECJQlQKAsAQJl\nCRAoS4BAWQIEyhIgUJYAgbIECJQlQKAsAQJlCRAoS4BAWQIEyhIgUJYAgbIECBZd6T8A//lmZmZy\n9sKFC3P++itXrszZRx99NOU++uiji/3jzGrks1q4cGHKnT179mL/OJfdyPuv5uo35ckSIFCWAIGy\nBAiUJUCgLAECZQkQKEuAQFkCBMoSIFCWAIF1R+bdggX93+Rz586l3O23355n7tixI2dPnDiRcseO\nHcszT548mbOff/55zs7HGuPIumH9Xkdmzsd7qmuh/x9PlgCBsgQIlCVAoCwBAmUJEChLgEBZAgTK\nEiBQlgCBDR7m3cgGRd3gefLJJ/PMp556Kmd//PHHlFuyZEmeuXz58px9+umnc/btt99OuYMHD+aZ\nI8e96nc14uqrr87Z8+fPp9zx48cv9o/zP3iyBAiUJUCgLAECZQkQKEuAQFkCBMoSIFCWAIGyBAiU\nJUBg3ZF5d/r06Tmfed999+Xsxo0bc7auZo4cYfvkk09y9p577snZN954I+V2796dZ+7duzdn9+3b\nl3L3339/njnyve7cuTPldu3alWfOxpMlQKAsAQJlCRAoS4BAWQIEyhIgUJYAgbIECJQlQKAsAQLr\njlyUmZmZnB25GFivG27ZsiXPPHLkSM6uWLEi5TZt2pRnjmS/+OKLnP36669TbuRi4kMPPZSzW7du\nTbkzZ87kmSPvf8eOHSl36tSpPHM2niwBAmUJEChLgEBZAgTKEiBQlgCBsgQIlCVAoCwBgpkLI+sV\nF/siA9sezL0r/fmP/MQ+++yzlBs5QjaiflZnz57NM+fjYNs0TdPJkydT7vz583nml19+mbN1g2jk\ns3r22Wdz9rbbbku5G2+8Mc+c7bfqyRIgUJYAgbIECJQlQKAsAQJlCRAoS4BAWQIEyhIgUJYAgYNl\n/wKXYaN1zhw6dCjl1q9fn2eeOHEiZ5csWZJyixb1vzojB8PqCuM0TdOyZctSbmTd8bHHHsvZhx9+\nOOUWLOjPZOvWrcvZjz/+OGfngidLgEBZAgTKEiBQlgCBsgQIlCVAoCwBAmUJEChLgEBZAgTWHflb\nWb58ecqNrNCNZI8fP55yhw8fzjP/+OOPnB25WlnXWEeue458VvW7OnfuXJ45spp588035+xc8GQJ\nEChLgEBZAgTKEiBQlgCBsgQIlCVAoCwBAmUJENjg+ReYjw2Oka2MkYNdGzZsSLlTp07lmSPZerDs\n9OnTeWbdCpqmabrmmmtytm4G1U2baZqmxYsX5+yRI0dSbvXq1Xnmnj17crb+rrZs2ZJnzsaTJUCg\nLAECZQkQKEuAQFkCBMoSIFCWAIGyBAiUJUCgLAEC647/AvWw1TRN08KFC1NuZN3xxRdfzNkbbrgh\n5X777bc8c9myZTlbD2atWLEizxw5rDWyRllXM8+cOZNnLlrUK6F+rmvWrMkz33rrrZzdvHlzyo28\np9l4sgQIlCVAoCwBAmUJEChLgEBZAgTKEiBQlgCBsgQIlCVAMHNhZBfuYl9k4Logc29k3evs2bNz\n/voPPPBAzn7wwQcpd+LEiTyzrnBOU1/jXLlyZZ558uTJnK0XG6dpmq666qo5zU3T2BrnoUOHcrYa\n+azefPPNlHvnnXfyzNnq0JMlQKAsAQJlCRAoS4BAWQIEyhIgUJYAgbIECJQlQPCPPVg2shU0ssGx\nYEH792Pk9UcORtWDWSPmYytnxIcffpizx44dS7mRDZ7FixfnbF1oGzmYNvL7W7p0ac6O/K7mY2b9\nrY68/7vvvjtnDx8+nLNzwZMlQKAsAQJlCRAoS4BAWQIEyhIgUJYAgbIECJQlQKAsAYK/3bpjXY2q\nh6Wm6cqv+11pjz/+eM6+8MILKffII4/kmcePH8/ZerBrZIVx5GBb/V2NvKeRdb8lS5bkbF2NHLlJ\nOPK+qpHv6ujRozm7devWlHv//ffzzNl4sgQIlCVAoCwBAmUJEChLgEBZAgTKEiBQlgCBsgQIlCVA\nMHNhZBfqYl9k4BLilXbdddel3IYNG/LMO+64I2fr3LrqNU3TtGnTppw9depUytUrmNM0djFw2bJl\nKffzzz/nmVdddVXO1tW8NWvW5JmnT5/O2eXLl+fszp07U+7qq6/OM0dWY+t1x5ErjCPf1cGDB1Pu\nrrvuyjNnq0NPlgCBsgQIlCVAoCwBAmUJEChLgEBZAgTKEiBQlgCBsgQI/nbrjg8++GDKvfrqq3nm\n9ddfn7PXXHNNyo1clxy57vfXX3+l3MjFypEVurqaN/KdnjhxImf37duXctu2bcszd+/enbMrV65M\nuWuvvTbP3LhxY86OOHDgQMrV9zRN03TkyJGcrZcg6wrrNI2tZq5atSrlRn7/1h0BLpGyBAiUJUCg\nLAECZQkQKEuAQFkCBMoSIFCWAMFl2eBZtGhRzu7atSvl1q9fn2eObNvUbN1eGFW3fUa2YubD6tWr\nc3bt2rU5+9JLL6XcM888k2e+/PLLOVsPoZ08eTLP/Oabb3K2buVMUz+EN1/H1epxsZENopGDZfVg\n2i233JJn2uABuETKEiBQlgCBsgQIlCVAoCwBAmUJEChLgEBZAgTKEiC4LOuO27dvz9nXX3895fbv\n359njhxBqtklS5bkmSPqutfIuuEPP/yQs3Xdb+QI3IIF/d/kG264IeWef/75PHPp0qU5W4+Ljfym\n7r333nnJ1s91ZIVx5LtavHhxzlYjh/Dq35V6BHGapun777//P/+bJ0uAQFkCBMoSIFCWAIGyBAiU\nJUCgLAECZQkQKEuAQFkCBP3s4iX49ddfc7au5o1cjDt16tScv/7IutvIWtiqVatS7s8//8wzv/vu\nu5yt72vkuuTIJcSzZ8+m3HvvvZdn7t27N2fruuN1112XZ46sG/711185e+bMmZSrn+k09YuJ09TX\nDUdmjqw71r9XmzZtyjNn48kSIFCWAIGyBAiUJUCgLAECZQkQKEuAQFkCBMoSILgsGzw//fRTztb7\naT/++GOeuWLFipxdu3Ztyo1sWvz+++85+9tvv6XcokX9qxs5rla3MkaOgI1sW9WDWSOf6V133ZWz\nx44dS7mRI3CHDh3K2ZHvqn4GddNnmsa2fercZcuW5Zn1YN00TdPhw4dTbvPmzXnmbDxZAgTKEiBQ\nlgCBsgQIlCVAoCwBAmUJEChLgEBZAgTKEiC4LOuOX331Vc6+++67Kbd9+/Y88+eff87ZAwcOpNzI\nEa6R42Z13XBkhWzkYNrChQtTbuQI3Llz53K2rrseP348z/zll1/m/PVH3tPIaup8/K7m62Bazc7X\nuuWtt96acgcPHswzZ+PJEiBQlgCBsgQIlCVAoCwBAmUJEChLgEBZAgTKEiBQlgDBzIW633UpLzIz\nM+czn3vuuZx95ZVXcnbdunUpN3JdcGSFrK7R1bXEaRpbd6yreSOvP/L9159jXQsdzdbPamTmfPz+\nR+bO1brf/1Y/q/Pnz+eZI9cd9+zZk3Lbtm3LM2f7/XmyBAiUJUCgLAECZQkQKEuAQFkCBMoSIFCW\nAIGyBAguywbPyLbHyP/tPx+eeOKJlHvttdfyzLoVNE3TtHr16pRbsKD/Ozfy+dcNnpGDXSN+/fXX\nlBv52f700085W39/R48ezTNHPv8R9TMYORg2cgiu/gY//fTTPHPfvn05u3PnzpytbPAAXCJlCRAo\nS4BAWQIEyhIgUJYAgbIECJQlQKAsAQJlCRD8Yw+W/ae68847U27t2rV55sjBtJtuuinlvv322zxz\nZN1u//79OQtzzbojwCVSlgCBsgQIlCVAoCwBAmUJEChLgEBZAgTKEiBQlgCBdUeA/2bdEeASKUuA\nQFkCBMoSIFCWAIGyBAiUJUCgLAECZQkQKEuAQFkCBMoSIFCWAIGyBAiUJUCgLAECZQkQKEuAQFkC\nBMoSIFh0OV7kMtxEA5hXniwBAmUJEChLgEBZAgTKEiBQlgCBsgQIlCVAoCwBAmUJEChLgEBZAgTK\nEiBQlgCBsgQIlCVAoCwBAmUJEChLgEBZAgTKEiBQlgCBsgQI/gskitpsejS6pwAAAABJRU5ErkJg\ngg==\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "8I1rONJT53Wu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Since the weights, and biases in the two linear layers [linear_fn1, linear_fn2] are not trained, the predictions are arbitrary at this point. One of the tasks for this assignment is for you to train the neural network."
      ]
    },
    {
      "metadata": {
        "id": "L_8eQuKBi7u0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Assignment Questions [10pts + 3pts extra]"
      ]
    },
    {
      "metadata": {
        "id": "HpPb-CRh47Ct",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* **[3pts] ** Provide code for the following activation functions:\n",
        "\n",
        "$$\\text{Sigmoid(x)} = \\frac{1}{1 + e^{-x}} = \\frac{e^x}{e^x + 1}$$\n",
        "\n",
        "$$\\text{Tanh(x)} = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$$\n",
        "\n",
        "$$ \\text{LeakyReLU}(x) = \\begin{cases} \n",
        "      0.01 x & x < 0 \\\\\n",
        "      x & x \\geq 0 \n",
        "\\end{cases}$$"
      ]
    },
    {
      "metadata": {
        "id": "0ReR6YZC47eW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "c0f2137e-ec23-49e9-a687-fdc1fddec190"
      },
      "cell_type": "code",
      "source": [
        "# Sigmoid of x.\n",
        "class toynn_Sigmoid:\n",
        "    def forward(self, x):\n",
        "        # Forward pass.\n",
        "        exp = x.exp()\n",
        "        self.sigmoid = exp / (exp + 1)\n",
        "        return self.sigmoid\n",
        "    \n",
        "    def backward(self, x, output_grads):\n",
        "        # Backward pass\n",
        "        return self.sigmoid * (1 - self.sigmoid) * output_grads\n",
        "        \n",
        "# Hyperbolic tangent.\n",
        "class toynn_Tanh:\n",
        "    def forward(self, x):\n",
        "        # Forward pass.\n",
        "        exp = x.exp()\n",
        "        m_exp = (-x).exp()\n",
        "        self.tanh = (exp - m_exp) / (exp + m_exp)\n",
        "        return self.tanh\n",
        "    \n",
        "    def backward(self, x, output_grads):\n",
        "        # Backward pass\n",
        "        return (1 - self.tanh ** 2) * output_grads\n",
        "      \n",
        "# LeakyReLU of x.\n",
        "class toynn_LeakyReLU:\n",
        "    def forward(self, x):\n",
        "        # Forward pass.\n",
        "        x[x<0] *= 0.01\n",
        "        return x\n",
        "    \n",
        "    def backward(self, x, output_grads):\n",
        "        input_grads = output_grads.clone()\n",
        "        input_grads[x < 0] *= 0.01\n",
        "        return input_grads\n",
        "      \n",
        "x = torch.randn(2, 3)\n",
        "grads = torch.randn(2, 3)\n",
        "\n",
        "activation_fn = toynn_Sigmoid()\n",
        "print('sigmoid forward')\n",
        "print(activation_fn.forward(x))\n",
        "print('sigmoid backward')\n",
        "print(activation_fn.backward(x, grads))\n",
        "\n",
        "activation_fn = toynn_Tanh()\n",
        "print('tanh forward')\n",
        "print(activation_fn.forward(x))\n",
        "print('tanh backward')\n",
        "print(activation_fn.backward(x, grads))\n",
        "\n",
        "activation_fn = toynn_LeakyReLU()\n",
        "print('LeakyReLU forward')\n",
        "print(activation_fn.forward(x))\n",
        "print('LeakyReLU backward')\n",
        "print(activation_fn.backward(x, grads))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sigmoid forward\n",
            "tensor([[0.3687, 0.5763, 0.1714],\n",
            "        [0.7518, 0.1261, 0.6631]])\n",
            "sigmoid backward\n",
            "tensor([[-0.1390,  0.3945,  0.1620],\n",
            "        [ 0.0957,  0.0129, -0.2322]])\n",
            "tanh forward\n",
            "tensor([[-0.4913,  0.2981, -0.9179],\n",
            "        [ 0.8035, -0.9592,  0.5896]])\n",
            "tanh backward\n",
            "tensor([[-0.4532,  1.4719,  0.1796],\n",
            "        [ 0.1817,  0.0094, -0.6782]])\n",
            "LeakyReLU forward\n",
            "tensor([[-0.0054,  0.3074, -0.0158],\n",
            "        [ 1.1083, -0.0194,  0.6770]])\n",
            "LeakyReLU backward\n",
            "tensor([[-5.9736e-03,  1.6155e+00,  1.1401e-02],\n",
            "        [ 5.1268e-01,  1.1732e-03, -1.0395e+00]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tAMnCGwv-zJN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* **[2pts] ** Provide code for the binary cross entropy loss function:\n",
        "\n",
        "$$\\ell(y, \\hat{y}) = -\\sum_{i=1}^{i=n} [y_i\\text{log}(\\hat{y}_i) + (1 - y_i)\\text{log}(1 - \\hat{y}_i)]$$,\n",
        "\n",
        "where $n$ is the number of outputs (e.g. the size of vectors $y$ and $\\hat{y}$), the entries in the target vector $y_i$ are binary $\\in \\{0,1\\}$ and $y_i$ are typically the outputs of a sigmoid layer. Remember that the backward pass does not return a scalar but a vector containing the values for $\\partial \\ell / \\partial \\hat{y}_i$, henceforth a vector of the same size as $\\hat{y}$."
      ]
    },
    {
      "metadata": {
        "id": "zVC2BJpa-5Ka",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "b7593a8e-9229-46e0-aff0-3d6c52401df0"
      },
      "cell_type": "code",
      "source": [
        "# Binary cross entropy loss. \n",
        "# Useful for classification when the classes are not mutually exclusive.\n",
        "# For instance, when both shoe, and dress, are correct labels for an image.\n",
        "# In other words, when images have multiple labels per image.\n",
        "class toynn_BCELoss:\n",
        "    def forward(self, predictions, targets):\n",
        "        # Forward pass.\n",
        "        loss = predictions.clone()\n",
        "        loss[targets == 1] = loss[targets == 1]\n",
        "        loss[targets == 0] = 1 - predictions[targets == 0]\n",
        "        return -(loss.log().sum(1)).mean()\n",
        "        \n",
        "        \n",
        "        \n",
        "    def backward(self, predictions, targets):\n",
        "        # Backward pass.\n",
        "        input_grads = predictions.clone()\n",
        "        input_grads[targets == 0] = input_grads[targets == 0] - 1\n",
        "        return -1 / input_grads\n",
        "        \n",
        "predictions = torch.rand(2, 3)\n",
        "targets = torch.LongTensor([[0, 1, 1], [1, 0, 1]])\n",
        "\n",
        "\n",
        "loss_fn = toynn_BCELoss()\n",
        "print('BCELoss forward')\n",
        "print(loss_fn.forward(predictions, targets))\n",
        "print('BCELoss backward')\n",
        "print(loss_fn.backward(predictions, targets))"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BCELoss forward\n",
            "tensor(2.5643)\n",
            "BCELoss backward\n",
            "tensor([[ 2.7931, -1.3312, -1.3156],\n",
            "        [-3.7936,  6.0194, -1.5109]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LGIxg3q5i_bl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* **[5pts] ** Train the two-layer Neural Network as defined in Section 3 of this Assignment on FashionMNIST using toynn. Include below the code for training this neural network and report the training, validation plots for loss and accuracy. The code should be similar to the code in Section 2 of this assignment, please follow that convention."
      ]
    },
    {
      "metadata": {
        "id": "UzQc9ZtKjnf0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "outputId": "e3a82887-1f92-4bd0-d386-692d3217fb35"
      },
      "cell_type": "code",
      "source": [
        "# Your code goes here.\n",
        "# Define a learning rate. \n",
        "learningRate = 1e-3\n",
        "\n",
        "# Define number of epochs.\n",
        "N = 5\n",
        "batchSize = 100\n",
        "\n",
        "# Setup the number of inputs, hidden neurons, and outputs.\n",
        "nInputs = 1 * 28 * 28\n",
        "nHidden = 512\n",
        "nOutputs = 10\n",
        "\n",
        "# Create the model.\n",
        "loss_fn = toynn_CrossEntropyLoss()\n",
        "linear_fn1 = toynn_Linear(nInputs, nHidden)\n",
        "activation_fn = toynn_ReLU() # toynn_Sigmoid toynn_Tanh toynn_LeakyReLU\n",
        "linear_fn2 = toynn_Linear(nHidden, nOutputs)\n",
        "\n",
        "\n",
        "# log accuracies and losses.\n",
        "train_accuracies = []; val_accuracies = []\n",
        "train_losses = []; val_losses = []\n",
        "\n",
        "# Training loop. Please make sure you understand every single line of code below.\n",
        "# Go back to some of the previous steps in this lab if necessary.\n",
        "for epoch in range(0, N):\n",
        "    correct = 0.0\n",
        "    cum_loss = 0.0\n",
        "    \n",
        "    # Make a pass over the training data.\n",
        "    for (i, (inputs, labels)) in enumerate(train_loader):\n",
        "        inputs = inputs.view(batchSize, 1 * 28 * 28)\n",
        "        \n",
        "        # Forward pass. (Prediction stage)\n",
        "        a = linear_fn1.forward(inputs)\n",
        "        z = activation_fn.forward(a)\n",
        "        yhat = linear_fn2.forward(z)\n",
        "        \n",
        "        cum_loss += loss_fn.forward(yhat, labels).item()\n",
        "        \n",
        "        # Count how many correct in this batch.\n",
        "        max_scores, max_labels = yhat.max(1)\n",
        "        correct += (max_labels == labels).sum().item()\n",
        "        \n",
        "        #Backward pass. (Gradient computation stage)\n",
        "        scores_grads = loss_fn.backward(scores, labels)\n",
        "        z_grads = linear_fn2.backward(z, scores_grads)\n",
        "        a_grads = activation_fn.backward(a, z_grads)\n",
        "        inputs_grads = linear_fn1.backward(inputs, a_grads)\n",
        "        \n",
        "        \n",
        "        # Parameter updates (SGD step).\n",
        "        linear_fn1.weight.add_(-learningRate, linear_fn1.weight_grads)\n",
        "        linear_fn1.bias.add_(-learningRate, linear_fn1.bias_grads)\n",
        "        linear_fn2.weight.add_(-learningRate, linear_fn2.weight_grads)\n",
        "        linear_fn2.bias.add_(-learningRate, linear_fn2.bias_grads)\n",
        "       \n",
        "    \n",
        "        # Logging the current results on training.\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print('Train-epoch %d. Iteration %05d, Avg-Loss: %.4f, Accuracy: %.4f' % \n",
        "                  (epoch, i + 1, cum_loss / (i + 1), correct / ((i + 1) * batchSize)))\n",
        "    \n",
        "    train_accuracies.append(correct / len(trainset))\n",
        "    train_losses.append(cum_loss / len(trainset))\n",
        "    \n",
        "    \n",
        "    # Make a pass over the validation data.\n",
        "    correct = 0.0\n",
        "    cum_loss = 0.0\n",
        "    for (i, (inputs, labels)) in enumerate(val_loader):\n",
        "        inputs = inputs.view(batchSize, 1 * 28 * 28)\n",
        "        \n",
        "        # Forward pass. (Prediction stage)\n",
        "        a = linear_fn1.forward(inputs)\n",
        "        z = activation_fn.forward(a)\n",
        "        yhat = linear_fn2.forward(z)\n",
        "        \n",
        "        cum_loss += loss_fn.forward(yhat, labels).item()\n",
        "        \n",
        "         # Count how many correct in this batch.\n",
        "        max_scores, max_labels = yhat.max(1)\n",
        "        correct += (max_labels == labels).sum().item()\n",
        "          \n",
        "    val_accuracies.append(correct / len(valset))\n",
        "    val_losses.append(cum_loss / (i + 1))\n",
        "            \n",
        "    # Logging the current results on validation.\n",
        "    print('Validation-epoch %d. Avg-Loss: %.4f, Accuracy: %.4f' % \n",
        "          (epoch, cum_loss / (i + 1), correct / len(valset)))"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train-epoch 0. Iteration 00100, Avg-Loss: 0.9968, Accuracy: 0.6554\n",
            "Train-epoch 0. Iteration 00200, Avg-Loss: 0.7915, Accuracy: 0.7178\n",
            "Train-epoch 0. Iteration 00300, Avg-Loss: 0.7071, Accuracy: 0.7465\n",
            "Train-epoch 0. Iteration 00400, Avg-Loss: 0.6547, Accuracy: 0.7641\n",
            "Train-epoch 0. Iteration 00500, Avg-Loss: 0.6223, Accuracy: 0.7751\n",
            "Train-epoch 0. Iteration 00600, Avg-Loss: 0.5985, Accuracy: 0.7837\n",
            "Validation-epoch 0. Avg-Loss: 0.4712, Accuracy: 0.8263\n",
            "Train-epoch 1. Iteration 00100, Avg-Loss: 0.4485, Accuracy: 0.8323\n",
            "Train-epoch 1. Iteration 00200, Avg-Loss: 0.4346, Accuracy: 0.8405\n",
            "Train-epoch 1. Iteration 00300, Avg-Loss: 0.4237, Accuracy: 0.8444\n",
            "Train-epoch 1. Iteration 00400, Avg-Loss: 0.4240, Accuracy: 0.8443\n",
            "Train-epoch 1. Iteration 00500, Avg-Loss: 0.4208, Accuracy: 0.8458\n",
            "Train-epoch 1. Iteration 00600, Avg-Loss: 0.4197, Accuracy: 0.8466\n",
            "Validation-epoch 1. Avg-Loss: 0.4211, Accuracy: 0.8445\n",
            "Train-epoch 2. Iteration 00100, Avg-Loss: 0.3901, Accuracy: 0.8555\n",
            "Train-epoch 2. Iteration 00200, Avg-Loss: 0.3831, Accuracy: 0.8597\n",
            "Train-epoch 2. Iteration 00300, Avg-Loss: 0.3821, Accuracy: 0.8601\n",
            "Train-epoch 2. Iteration 00400, Avg-Loss: 0.3745, Accuracy: 0.8622\n",
            "Train-epoch 2. Iteration 00500, Avg-Loss: 0.3765, Accuracy: 0.8622\n",
            "Train-epoch 2. Iteration 00600, Avg-Loss: 0.3739, Accuracy: 0.8633\n",
            "Validation-epoch 2. Avg-Loss: 0.4224, Accuracy: 0.8429\n",
            "Train-epoch 3. Iteration 00100, Avg-Loss: 0.3567, Accuracy: 0.8733\n",
            "Train-epoch 3. Iteration 00200, Avg-Loss: 0.3528, Accuracy: 0.8718\n",
            "Train-epoch 3. Iteration 00300, Avg-Loss: 0.3548, Accuracy: 0.8716\n",
            "Train-epoch 3. Iteration 00400, Avg-Loss: 0.3507, Accuracy: 0.8737\n",
            "Train-epoch 3. Iteration 00500, Avg-Loss: 0.3497, Accuracy: 0.8737\n",
            "Train-epoch 3. Iteration 00600, Avg-Loss: 0.3488, Accuracy: 0.8736\n",
            "Validation-epoch 3. Avg-Loss: 0.3898, Accuracy: 0.8522\n",
            "Train-epoch 4. Iteration 00100, Avg-Loss: 0.3236, Accuracy: 0.8790\n",
            "Train-epoch 4. Iteration 00200, Avg-Loss: 0.3213, Accuracy: 0.8800\n",
            "Train-epoch 4. Iteration 00300, Avg-Loss: 0.3211, Accuracy: 0.8811\n",
            "Train-epoch 4. Iteration 00400, Avg-Loss: 0.3255, Accuracy: 0.8804\n",
            "Train-epoch 4. Iteration 00500, Avg-Loss: 0.3263, Accuracy: 0.8804\n",
            "Train-epoch 4. Iteration 00600, Avg-Loss: 0.3264, Accuracy: 0.8804\n",
            "Validation-epoch 4. Avg-Loss: 0.3605, Accuracy: 0.8694\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gXlKZ2eQj9Y7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* **[3pts (extra)]** Train a Neural Network **using toynn** on the CIFAR 10 Dataset (torchvision.datasets.CIFAR10). Include the code for defining, and training the neural network, as well as plots for training, validation for loss and accuracy. Show the predictions for one example of this dataset. Write down below your code in bold face the accuracy you obtained and in how many epochs. I will post the highest accuracies that people obtained for this part."
      ]
    },
    {
      "metadata": {
        "id": "feWB2DO7lWQv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Your code goes here."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7VLqNpM79-3l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**I obtained a validation accuracy of XX% after X epochs.**"
      ]
    }
  ]
}